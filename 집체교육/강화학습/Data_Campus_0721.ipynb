{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1bPMaL7fWqQL7tf6Tyk8s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snmOKH7mFEtB","outputId":"c3662499-8a42-4ebd-fe09-bde3618d9b36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Route:\n"]}],"source":["# AI for Logistics - Robots in a warehouse\n","# Importing the libraries\n","import numpy as np\n","# Setting the parameters gamma and alpha for the Q-Learning\n","gamma = 0.75\n","alpha = 0.9\n","# PART 1 - BUILDING THE ENVIRONMENT\n","# Defining the states\n","location_to_state = {'A': 0,'B': 1,'C': 2,'D': 3,'E': 4,'F': 5,'G': 6,'H': 7,'I': 8,'J': 9,'K': 10,'L': 11}\n","# Defining the actions\n","actions = [0,1,2,3,4,5,6,7,8,9,10,11]\n","# Defining the rewards\n","R = np.array([\n","    [0,1,0,0,0,0,0,0,0,0,0,0],\n","    [1,0,1,0,0,1,0,0,0,0,0,0],\n","    [0,1,0,0,0,0,1,0,0,0,0,0],\n","    [0,0,0,0,0,0,0,1,0,0,0,0],\n","    [0,0,0,0,0,0,0,0,1,0,0,0],\n","    [0,1,0,0,0,0,0,0,0,1,0,0],\n","    [0,0,1,0,0,0,1,1,0,0,0,0],\n","    [0,0,0,1,0,0,1,0,0,0,0,1],\n","    [0,0,0,0,1,0,0,0,0,1,0,0],\n","    [0,0,0,0,0,1,0,0,1,0,1,0],\n","    [0,0,0,0,0,0,0,0,0,1,0,1],\n","    [0,0,0,0,0,0,0,1,0,0,1,0]])\n","# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n","# Making a mapping from the states to the locations\n","state_to_location = {state: location for location, state in location_to_state.items()}\n","# Making a function that returns the shortest route from a starting to ending location\n","def route(starting_location, ending_location):\n","    R_new = np.copy(R)\n","    ending_state = location_to_state[ending_location]\n","    R_new[ending_state, ending_state] = 1000\n","    Q = np.array(np.zeros([12,12]))\n","    for i in range(1000):\n","        current_state = np.random.randint(0,12)\n","        playable_actions = []\n","        for j in range(12):\n","            if R_new[current_state, j] > 0:\n","                playable_actions.append(j)\n","        next_state = np.random.choice(playable_actions)\n","        TD = R_new[current_state,next_state] + gamma * Q[next_state,np.argmax(Q[next_state,])] - Q[current_state, next_state]\n","        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n","        route = [starting_location]\n","        next_location = starting_location\n","        while (next_location != ending_location):\n","            starting_state = location_to_state[starting_location]\n","            next_state = np.argmax(Q[starting_state,])\n","            next_location = state_to_location[next_state]\n","            route.append(next_location)\n","            starting_location = next_location\n","    return route\n","# PART 3 - GOING INTO PRODUCTION\n","# Making the final function that returns the optimal route\n","def best_route(starting_location, intermediary_location, ending_location):\n","    return route(starting_location,intermediary_location) + route(intermediary_location, ending_location)[1:]\n","# Printing the final route\n","print('Route:')\n","best_route('E', 'B', 'G')"]}]}